{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chainsword\\AppData\\Local\\Temp\\ipykernel_19232\\2469337221.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from liftoff import parse_opts\n",
    "from argparse import Namespace\n",
    "from experiment_src import *\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(\".\"))))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from experiment_src import (\n",
    "    train_net_with_neural_fitting,\n",
    "    generate_random_policy_transitions,\n",
    "    generate_transitions_observations,\n",
    "    TransitionDataset\n",
    ")\n",
    "from experiments.experiment_utils import setup_logger, seed_everything\n",
    "from overfitting.src.policy_iteration import random_policy_evaluation_q_stochastic\n",
    "from overfitting.src.utils import (\n",
    "    create_random_policy,\n",
    "    extract_V_from_Q_for_stochastic_policy,\n",
    ")\n",
    "from overfitting.src.visualize import draw_simple_gridworld\n",
    "from experiment_src import generate_train_test_split_with_valid_path, check_path_existence_to_any_terminal\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences_and_compute_percentage(\n",
    "    sampled_transitions_list, total_unique_transitions, N\n",
    "):\n",
    "    # Count occurrences of each index in the sampled list\n",
    "    occurrences_count = {}\n",
    "    for index in sampled_transitions_list:\n",
    "        occurrences_count[index] = occurrences_count.get(index, 0) + 1\n",
    "\n",
    "    # Compute the number of indexes that appear at least N times\n",
    "    at_least_N = sum(1 for count in occurrences_count.values() if count >= N)\n",
    "\n",
    "    # Compute the percentage relative to the total number of unique transitions\n",
    "    percentage = (at_least_N / total_unique_transitions) * 100\n",
    "    return percentage, occurrences_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 10, 10\n",
    "start_state = (0, 0)\n",
    "terminal_states = {(rows - 2, cols - 2): 1.0}\n",
    "p_success = 1\n",
    "seed = 3\n",
    "run_id = 0\n",
    "\n",
    "num_steps = 40_000\n",
    "min_samples = 20\n",
    "# min_samples = 0\n",
    "\n",
    "# Learning hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.05  # Convergence criterion\n",
    "tau = 100\n",
    "batch_size = 32\n",
    "train_max_iterations = 50\n",
    "theta = 1e-6\n",
    "\n",
    "env = make_env(rows, cols, start_state, p_success, terminal_states, seed)\n",
    "\n",
    "states = list(set([s for s, _ in env.mdp.keys()]))\n",
    "actions = list(set([a for _, a in env.mdp.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1000\n",
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "transitions_train, transitions_val = train_test_split(\n",
    "    transitions_list, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "random_policy_transitions = generate_transitions_observations(\n",
    "    transitions_list,\n",
    "    num_steps,\n",
    "    tau=tau,\n",
    "    min_samples=min_samples,\n",
    ")\n",
    "\n",
    "\n",
    "### Training\n",
    "input_size = len(states[0])  # Or another way to represent the size of your input\n",
    "output_size = len(actions)\n",
    "\n",
    "# Initialize the DQN\n",
    "qnet_random_policy = QNET(input_size, output_size)\n",
    "\n",
    "# loss_record_random_policy = train_net_with_neural_fitting(\n",
    "#     qnet_random_policy,\n",
    "#     random_policy_transitions,\n",
    "#     states,\n",
    "#     actions,\n",
    "#     gamma,\n",
    "#     epsilon,\n",
    "#     batch_size,\n",
    "#     train_max_iterations,\n",
    "#     logger,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Initialize the DQN\u001b[39;00m\n\u001b[0;32m     32\u001b[0m qnet_random_policy \u001b[38;5;241m=\u001b[39m QNET(input_size, output_size)\n\u001b[1;32m---> 34\u001b[0m loss_record \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_net_with_neural_fitted_q\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqnet_random_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_policy_transitions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mQ_pi_random\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_max_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_scaling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\repos\\phd-research\\experiments\\experiment_distributions\\experiment_src.py:229\u001b[0m, in \u001b[0;36mtrain_net_with_neural_fitted_q\u001b[1;34m(net, transitions, Q_pi_random, states, actions, gamma, epsilon, batch_size, max_iterations, frequency_scaling, logger)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, action, next_state, reward, done, scale_factor \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m    227\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 229\u001b[0m     bellmans_errors \u001b[38;5;241m=\u001b[39m \u001b[43mbellman_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m     net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frequency_scaling:\n",
      "File \u001b[1;32md:\\Work\\repos\\phd-research\\experiments\\experiment_distributions\\experiment_src.py:315\u001b[0m, in \u001b[0;36mbellman_error\u001b[1;34m(model, states, actions, next_states, rewards, dones, mode, gamma)\u001b[0m\n\u001b[0;32m    312\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Ensure the model is in evaluation mode for this calculation\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;66;03m# Predict Q-values for all states and next states\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m model(next_states)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# Compute max or mean Q-value for next states based on mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\repos\\phd-research\\experiments\\experiment_distributions\\experiment_src.py:349\u001b[0m, in \u001b[0;36mQNET.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Chainsword\\anaconda3\\envs\\phd_rl_algos\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed_everything(run_id)\n",
    "env = make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n",
    "\n",
    "states = list(set([s for s, _ in env.mdp.keys()]))\n",
    "actions = list(set([a for _, a in env.mdp.keys()]))\n",
    "random_policy = create_random_policy(states, actions)\n",
    "Q = {state: {action: 0 for action in actions} for state in states}\n",
    "Q_pi_random = random_policy_evaluation_q_stochastic(\n",
    "    states, actions, random_policy, Q, env.mdp, gamma, epsilon\n",
    ")\n",
    "\n",
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "\n",
    "transitions_train, transitions_val = generate_train_test_split_with_valid_path(\n",
    "    transitions_list=transitions_list,\n",
    "    start_state=start_state,\n",
    "    terminal_states=terminal_states,\n",
    "    seed=run_id,\n",
    ")\n",
    "\n",
    "random_policy_transitions = generate_random_policy_transitions(\n",
    "    transitions_train, num_steps, env, actions\n",
    ")\n",
    "\n",
    "seed_everything(seed)\n",
    "\n",
    "### Training\n",
    "input_size = len(states[0])  # Or another way to represent the size of your input\n",
    "output_size = len(actions)\n",
    "\n",
    "# Initialize the DQN\n",
    "qnet_random_policy = QNET(input_size, output_size)\n",
    "\n",
    "loss_record = train_net_with_neural_fitted_q(\n",
    "    qnet_random_policy,\n",
    "    random_policy_transitions,\n",
    "    Q_pi_random,\n",
    "    states,\n",
    "    actions,\n",
    "    gamma,\n",
    "    epsilon,\n",
    "    batch_size=batch_size,\n",
    "    max_iterations=train_max_iterations,\n",
    "    frequency_scaling=False,\n",
    "    logger=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10\n",
    "transitions = random_policy_transitions\n",
    "\n",
    "net = QNET(input_size, output_size)\n",
    "if logger is None:\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "net.train()\n",
    "dataset = TransitionDataset(transitions)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(max_iterations):\n",
    "    total_loss = 0\n",
    "    for state, action, next_state, reward, done in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        q_values = net(state)\n",
    "        next_q_values = net(next_state)\n",
    "        max_next_q_values = next_q_values.detach().max(1)[0]\n",
    "\n",
    "        target_q_values_for_actions = reward + gamma * max_next_q_values * (~done)\n",
    "\n",
    "        action_q_values = q_values.gather(1, action.unsqueeze(-1))\n",
    "\n",
    "        loss = loss_fn(action_q_values, target_q_values_for_actions.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7741, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action.unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2938e+00,  1.4427e-01, -3.4196e-01, -4.9531e-01],\n",
       "        [-5.5965e-01,  1.5732e-01, -1.1921e-01, -1.1361e-01],\n",
       "        [-9.2899e-01,  1.1963e-01, -6.7880e-02, -8.0473e-01],\n",
       "        [-1.0815e+00,  1.6027e-01, -2.7088e-01, -4.0533e-01],\n",
       "        [-9.2247e-01,  1.2536e-01, -1.3494e-01, -6.8017e-01],\n",
       "        [-1.7271e+00,  1.5155e-01, -5.4521e-01, -1.4430e-01],\n",
       "        [-1.3387e+00,  2.6558e-01, -4.1004e-01, -3.6590e-02],\n",
       "        [-1.1518e+00,  2.8918e-01, -3.6429e-01,  4.8384e-03],\n",
       "        [-1.6530e+00,  3.3030e-01, -5.4903e-01, -2.6282e-02],\n",
       "        [-1.3390e+00,  1.5814e-01, -3.4963e-01, -6.2659e-01],\n",
       "        [-1.1150e+00,  1.6729e-01, -2.7006e-01, -5.3678e-01],\n",
       "        [-8.8873e-01,  1.4126e-01, -2.3497e-01, -1.3536e-01],\n",
       "        [-6.0195e-01,  1.5063e-01, -9.8310e-04, -4.7670e-01],\n",
       "        [-5.3520e-01,  1.4904e-01,  7.8122e-03, -3.8799e-01],\n",
       "        [-1.4364e+00,  1.0457e-01, -3.8646e-01, -4.6702e-01],\n",
       "        [-5.2487e-01,  1.6767e-01, -2.9157e-02, -2.6788e-01],\n",
       "        [-1.6299e+00,  4.7175e-01, -5.7502e-01, -6.1003e-02],\n",
       "        [-1.3387e+00,  2.6558e-01, -4.1004e-01, -3.6590e-02],\n",
       "        [-7.7704e-01,  1.5174e-01, -1.8372e-01, -1.9967e-01],\n",
       "        [-4.9004e-01,  2.9195e-01, -1.1522e-01,  4.0072e-02],\n",
       "        [-9.1221e-01,  1.6401e-01, -1.9698e-01, -4.5045e-01],\n",
       "        [-9.9870e-01,  1.3630e-01, -2.4932e-01, -2.8948e-01],\n",
       "        [-7.2582e-01,  1.2560e-01, -5.4534e-02, -5.3815e-01],\n",
       "        [-1.3755e+00,  1.6481e-01, -3.4701e-01, -7.5710e-01],\n",
       "        [-8.5865e-01,  1.2033e-01, -6.3097e-02, -7.1554e-01],\n",
       "        [-8.2208e-01,  1.5366e-01, -3.1560e-02, -7.3579e-01],\n",
       "        [-8.0180e-01,  3.8339e-01, -2.5688e-01, -2.3551e-03],\n",
       "        [-1.1112e+00,  1.2911e-01, -2.9751e-01, -2.2445e-01],\n",
       "        [-1.5561e+00,  9.4371e-02, -4.2718e-01, -4.1377e-01],\n",
       "        [-1.1290e+00,  4.5303e-01, -3.7686e-01, -4.7140e-02],\n",
       "        [-1.0108e+00,  1.4817e-01, -2.9950e-01, -7.0831e-02],\n",
       "        [-1.1518e+00,  2.8918e-01, -3.6429e-01,  4.8384e-03]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_next_q_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmax_next_q_values\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m~\u001b[39mdone\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_next_q_values' is not defined"
     ]
    }
   ],
   "source": [
    "max_next_q_values * (~done.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3246],\n",
       "        [-0.7336],\n",
       "        [ 0.6904],\n",
       "        [ 0.0315],\n",
       "        [ 1.4223],\n",
       "        [ 0.0071],\n",
       "        [-0.0206],\n",
       "        [-1.7084],\n",
       "        [ 0.1359],\n",
       "        [ 1.1714],\n",
       "        [ 0.1897],\n",
       "        [ 0.0509],\n",
       "        [ 1.7091],\n",
       "        [ 0.0664],\n",
       "        [-1.0759],\n",
       "        [ 0.8194],\n",
       "        [-1.2782],\n",
       "        [-0.4596],\n",
       "        [ 0.6318],\n",
       "        [-0.0032],\n",
       "        [-1.5888],\n",
       "        [ 0.8822],\n",
       "        [-0.1998],\n",
       "        [-1.2811],\n",
       "        [-1.3465],\n",
       "        [ 0.9563],\n",
       "        [ 0.4550],\n",
       "        [ 0.8518],\n",
       "        [-0.6650],\n",
       "        [ 1.4859],\n",
       "        [-1.2025],\n",
       "        [ 0.2654]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3, 2, 3, 2, 2, 1, 2, 0, 2, 2, 0, 2, 1, 3, 1, 1, 2, 2, 1, 3, 1, 1,\n",
       "        1, 2, 2, 0, 1, 0, 1, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(state).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from liftoff import parse_opts\n",
    "from argparse import Namespace\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.realpath(\".\")))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from common.src.distribution_src import (\n",
    "    train_net_with_neural_fitting,\n",
    "    generate_random_policy_transitions,\n",
    "    generate_transitions_observations,\n",
    "    TransitionDataset,\n",
    "    generate_train_test_split_with_valid_path,\n",
    "    check_path_existence_to_any_terminal,\n",
    "    get_frequency_scaling, normalize_frequencies,\n",
    "    make_env\n",
    ")\n",
    "from common.src.experiment_utils import setup_logger, seed_everything\n",
    "from common.src.policy_iteration import random_policy_evaluation_q_stochastic\n",
    "from common.src.utils import (\n",
    "    create_random_policy,\n",
    "    extract_V_from_Q_for_stochastic_policy,\n",
    ")\n",
    "from common.src.models import QNET\n",
    "\n",
    "\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences_and_compute_percentage(\n",
    "    sampled_transitions_list, total_unique_transitions, N\n",
    "):\n",
    "    # Count occurrences of each index in the sampled list\n",
    "    occurrences_count = {}\n",
    "    for index in sampled_transitions_list:\n",
    "        occurrences_count[index] = occurrences_count.get(index, 0) + 1\n",
    "\n",
    "    # Compute the number of indexes that appear at least N times\n",
    "    at_least_N = sum(1 for count in occurrences_count.values() if count >= N)\n",
    "\n",
    "    # Compute the percentage relative to the total number of unique transitions\n",
    "    percentage = (at_least_N / total_unique_transitions) * 100\n",
    "    return percentage, occurrences_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = 10, 10\n",
    "start_state = (0, 0)\n",
    "terminal_states = {(rows - 2, cols - 2): 1.0}\n",
    "p_success = 1\n",
    "seed = 3\n",
    "run_id = 0\n",
    "\n",
    "num_steps = 40_000\n",
    "min_samples = 20\n",
    "# min_samples = 0\n",
    "\n",
    "# Learning hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.05  # Convergence criterion\n",
    "tau = 100\n",
    "batch_size = 32\n",
    "train_max_iterations = 50\n",
    "theta = 1e-6\n",
    "\n",
    "env = make_env(rows, cols, start_state, p_success, terminal_states, seed)\n",
    "\n",
    "states = list(set([s for s, _ in env.mdp.keys()]))\n",
    "actions = list(set([a for _, a in env.mdp.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1000\n",
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "transitions_train, transitions_val = train_test_split(\n",
    "    transitions_list, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "random_policy_transitions = generate_transitions_observations(\n",
    "    transitions_list,\n",
    "    num_steps,\n",
    "    tau=tau,\n",
    "    min_samples=min_samples,\n",
    ")\n",
    "\n",
    "\n",
    "### Training\n",
    "input_size = len(states[0])  # Or another way to represent the size of your input\n",
    "output_size = len(actions)\n",
    "\n",
    "# Initialize the DQN\n",
    "qnet_random_policy = QNET(input_size, output_size)\n",
    "\n",
    "# loss_record_random_policy = train_net_with_neural_fitting(\n",
    "#     qnet_random_policy,\n",
    "#     random_policy_transitions,\n",
    "#     states,\n",
    "#     actions,\n",
    "#     gamma,\n",
    "#     epsilon,\n",
    "#     batch_size,\n",
    "#     train_max_iterations,\n",
    "#     logger,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  20   20   21   20   41   95   22   20  557   20  103  164  217  209\n",
      "  857  292  440   20   48   20   27   54  220  103  148   39   25   21\n",
      "   21   28   89   22  511   29   64   93  704  729   20  843  253   24\n",
      "   20   21   50   55   20  688   20   22   20   46   22   23   20   36\n",
      "   20   20   34   31   40  704   22   20  287   22   25   20  242   20\n",
      "   53   23   21   22  261  333  179   20   21   20   20   23   41   24\n",
      "   20  673   20   23   75   20  666   20   24  161   41   20   24   21\n",
      "   20   24  618   20   21  191   20   74   20   20  171   23   22   20\n",
      "   20   34   47  960   40   31  635   20   20   38   22   20   52   20\n",
      "   86   20   55   53  952  150   78   20   42  561   24   20   20   20\n",
      "  226  100   20  570   35   20   41  310   22   23   68   22   20   96\n",
      "   20   20   32   64  687   20   27   20   20  411   79  828  650   91\n",
      "   20  904   20   21   30   44   24   21  799   20   20  391   20   60\n",
      "  543   20   34   23  435   70   20   34   20   28  865  119   27  488\n",
      "   20  232   21   20   21   47   27  783   38  611   23  295   21   20\n",
      "  303   81   63   30   20   20   33   28   57   61   26   23   32   23\n",
      "   20   21   21   20   30   26  316  557   20   21   20   52   20   37\n",
      "  388   72   24  138   45   20   20   20   28  287   40   34   21  130\n",
      "   63  148   20   39   21   63   43   31   87  180   40   20  283   74\n",
      "   22   20   80   29   25   40   22   20 1046   40   37   20   21   44\n",
      "   20  323   20   70   20   63  367   20  149   20   40   51   21   21\n",
      "   29   20  145   20   20   20  843  130   20   23   24   20   38   20\n",
      "  106  204   20  540   21  788   20   21]\n",
      "\n",
      "[127 127 127 127 127 126 126 127 127 127 126 127 126 126 127 126 127 127\n",
      " 126 127 126 127 127 127 126 127 126 126 126 127 127 127 126 126 126 126\n",
      " 127 126 126 126 127 127 127 127 127 127 126 127 127 126 127 127 127 127\n",
      " 126 127 126 126 126 127 127 126 127 126 127 126 126 127 126 127 127 127\n",
      " 127 127 127 127 126 127 127 127 126 127 127 126 127 127 127 126 126 127\n",
      " 127 127 127 126 126 126 127 126 126 127 127 127 127 126 127 127 126 127\n",
      " 127 126 127 127 127 127 126 127 126 127 127 127 126 127 126 127 127 126\n",
      " 127 126 127 127 127 126 127 127 127 126 127 126 127 127 126 126 126 127\n",
      " 126 127 126 126 127 127 126 127 126 127 127 127 126 127 127 126 127 127\n",
      " 127 127 127 127 127 126 126 127 126 127 127 127 126 127 127 126 126 127\n",
      " 126 126 127 126 127 127 126 126 126 127 127 126 127 127 126 126 127 126\n",
      " 126 127 127 127 127 126 127 127 126 127 127 127 126 127 127 126 127 126\n",
      " 127 126 126 127 127 127 126 126 126 126 126 127 127 127 126 127 126 126\n",
      " 127 127 126 126 126 126 127 126 126 127 126 127 127 127 127 126 127 127\n",
      " 127 127 127 126 126 126 127 127 127 126 126 126 127 127 126 126 126 126\n",
      " 127 127 126 127 126 127 126 127 126 126 127 127 126 126 126 127 127 127\n",
      " 126 127 126 126 127 127 126 127 126 127 127 126 127 126 127 127 126 127\n",
      " 127 126 126 126 127 127 126 127 127 127]\n"
     ]
    }
   ],
   "source": [
    "tau = 0.1\n",
    "seed_everything(run_id)\n",
    "env = make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n",
    "\n",
    "states = list(set([s for s, _ in env.mdp.keys()]))\n",
    "actions = list(set([a for _, a in env.mdp.keys()]))\n",
    "random_policy = create_random_policy(states, actions)\n",
    "Q = {state: {action: 0 for action in actions} for state in states}\n",
    "Q_pi_random = random_policy_evaluation_q_stochastic(\n",
    "    states, actions, random_policy, Q, env.mdp, gamma, epsilon\n",
    ")\n",
    "\n",
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "\n",
    "transitions_train, transitions_val = generate_train_test_split_with_valid_path(\n",
    "    transitions_list=transitions_list,\n",
    "    start_state=start_state,\n",
    "    terminal_states=terminal_states,\n",
    "    seed=run_id,\n",
    ")\n",
    "\n",
    "train_dataset_transitions = generate_transitions_observations(\n",
    "    transitions_train, num_steps, tau=tau, min_samples=min_samples\n",
    ")\n",
    "from scipy.stats import entropy\n",
    "\n",
    "freq_factors = get_frequency_scaling(train_dataset_transitions)\n",
    "# print(freq_factors)\n",
    "examples_normalized = [\n",
    "    (transition[0], transition[1]) for transition in train_dataset_transitions\n",
    "]\n",
    "example_strings_normalized = [\n",
    "    f\"{state}_{action}\" for state, action in examples_normalized\n",
    "]\n",
    "unique_examples, counts = np.unique(example_strings_normalized, return_counts=True)\n",
    "print(counts)\n",
    "\n",
    "print()\n",
    "\n",
    "normed_dataset = normalize_frequencies(train_dataset_transitions)\n",
    "normed_freq_factors = get_frequency_scaling(normed_dataset)\n",
    "# print(normed_freq_factors)\n",
    "examples_normalized = [(transition[0], transition[1]) for transition in normed_dataset]\n",
    "example_strings_normalized = [\n",
    "    f\"{state}_{action}\" for state, action in examples_normalized\n",
    "]\n",
    "unique_examples, counts = np.unique(example_strings_normalized, return_counts=True)\n",
    "print(counts)\n",
    "\n",
    "\n",
    "# random_policy_transitions = generate_random_policy_transitions(\n",
    "#     transitions_train, num_steps, env, actions\n",
    "# )\n",
    "\n",
    "# seed_everything(seed)\n",
    "\n",
    "# ### Training\n",
    "# input_size = len(states[0])  # Or another way to represent the size of your input\n",
    "# output_size = len(actions)\n",
    "\n",
    "# # Initialize the DQN\n",
    "# qnet_random_policy = QNET(input_size, output_size)\n",
    "\n",
    "# loss_record = train_net_with_neural_fitted_q(\n",
    "#     qnet_random_policy,\n",
    "#     random_policy_transitions,\n",
    "#     Q_pi_random,\n",
    "#     states,\n",
    "#     actions,\n",
    "#     gamma,\n",
    "#     epsilon,\n",
    "#     batch_size=batch_size,\n",
    "#     max_iterations=train_max_iterations,\n",
    "#     frequency_scaling=False,\n",
    "#     logger=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m transitions \u001b[38;5;241m=\u001b[39m random_policy_transitions\n\u001b[0;32m      4\u001b[0m net \u001b[38;5;241m=\u001b[39m QNET(input_size, output_size)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlogger\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m      8\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'logger' is not defined"
     ]
    }
   ],
   "source": [
    "max_iterations = 10\n",
    "transitions = random_policy_transitions\n",
    "\n",
    "net = QNET(input_size, output_size)\n",
    "if logger is None:\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "net.train()\n",
    "dataset = TransitionDataset(transitions)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(max_iterations):\n",
    "    total_loss = 0\n",
    "    for state, action, next_state, reward, done in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        q_values = net(state)\n",
    "        next_q_values = net(next_state)\n",
    "        max_next_q_values = next_q_values.detach().max(1)[0]\n",
    "\n",
    "        target_q_values_for_actions = reward + gamma * max_next_q_values * (~done)\n",
    "\n",
    "        action_q_values = q_values.gather(1, action.unsqueeze(-1))\n",
    "\n",
    "        loss = loss_fn(action_q_values, target_q_values_for_actions.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7741, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action.unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits, tau):\n",
    "    logits = np.array(logits)  # Ensure logits is a NumPy array for consistency\n",
    "    logits -= np.max(logits)  # Improves numerical stability\n",
    "    exp_logits = np.exp(logits / tau)\n",
    "    softmax_probs = exp_logits / np.sum(exp_logits)\n",
    "    return softmax_probs\n",
    "\n",
    "\n",
    "def generate_transitions_observations(\n",
    "    transitions_list, num_steps, tau, min_samples=None\n",
    "):\n",
    "    dset_size = len(transitions_list)\n",
    "\n",
    "    # Validate min_samples if provided\n",
    "    if min_samples is not None:\n",
    "        if not isinstance(min_samples, int):\n",
    "            raise ValueError(\"min_samples must be an integer\")\n",
    "        if min_samples * dset_size > num_steps:\n",
    "            raise ValueError(\n",
    "                \"min_samples times length of transitions exceeds num_steps\"\n",
    "            )\n",
    "\n",
    "        # Directly select each element in transitions_list min_samples times, considering their structure\n",
    "        repeated_transitions = [\n",
    "            transition for transition in transitions_list for _ in range(min_samples)\n",
    "        ]\n",
    "        remaining_steps = num_steps - (min_samples * dset_size)\n",
    "    else:\n",
    "        repeated_transitions = []\n",
    "        remaining_steps = num_steps\n",
    "\n",
    "    sampled_transitions = (\n",
    "        repeated_transitions  # Start with the manually repeated transitions\n",
    "    )\n",
    "\n",
    "    if remaining_steps > 0:\n",
    "        # Prepare logits for the softmax sampling; logits could be based on some criteria or just random\n",
    "        logits = np.random.uniform(0, 1, size=dset_size)\n",
    "        prob_dist = softmax(logits, tau)\n",
    "\n",
    "        # Sample the remaining transitions based on the softmax distribution\n",
    "        sampled_indices = np.random.choice(\n",
    "            dset_size, size=remaining_steps, p=prob_dist, replace=True\n",
    "        )\n",
    "        sampled_transitions += [transitions_list[i] for i in sampled_indices]\n",
    "\n",
    "    return sampled_transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_scaling(gen):\n",
    "    transition_counts = Counter(gen)\n",
    "\n",
    "    # Calculate expected frequency under uniform distribution\n",
    "    N_total = len(transitions)\n",
    "    N_unique = len(set(gen))\n",
    "    expected_frequency = N_total / N_unique\n",
    "\n",
    "    # Compute scaling factor relative to uniform distribution\n",
    "    inverse_frequency_scaling = {\n",
    "        t: expected_frequency / count for t, count in transition_counts.items()\n",
    "    }\n",
    "    return inverse_frequency_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "transitions = [i for i in range(400)]\n",
    "\n",
    "gen = generate_transitions_observations(\n",
    "    transitions, num_steps=40_000, tau=1e30, min_samples=10\n",
    ")\n",
    "scaling = Counter(gen)\n",
    "\n",
    "minimum_freq = min(scaling.values())\n",
    "maximum_freq = max(scaling.values())\n",
    "\n",
    "print(minimum_freq)\n",
    "print(maximum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00249136, 0.00250664, 0.00249756, 0.00249202, 0.00249281,\n",
       "       0.00248853, 0.0024886 , 0.00250517, 0.00250314, 0.00250934,\n",
       "       0.00250584, 0.00251034, 0.00251067, 0.00248909, 0.00250112,\n",
       "       0.00250186, 0.00248831, 0.00248776, 0.00250646, 0.00250619,\n",
       "       0.00250725, 0.00250653, 0.00250277, 0.00249907, 0.00250682,\n",
       "       0.00250226, 0.00251183, 0.00248767, 0.00250952, 0.00248894,\n",
       "       0.00251227, 0.0024928 , 0.00250414, 0.00250495, 0.00249739,\n",
       "       0.00249633, 0.00249662, 0.00250939, 0.00249308, 0.00250685,\n",
       "       0.00249795, 0.00250527, 0.00249277, 0.00249339, 0.00250964,\n",
       "       0.00250537, 0.00251213, 0.00249768, 0.00250348, 0.00250299,\n",
       "       0.00249657, 0.00250805, 0.00250233, 0.00251197, 0.00250898,\n",
       "       0.0025039 , 0.00250408, 0.00249371, 0.00249871, 0.00249676,\n",
       "       0.00250539, 0.00249965, 0.00249966, 0.00250149, 0.00249759,\n",
       "       0.00249083, 0.00250176, 0.00251129, 0.00250244, 0.00249053,\n",
       "       0.00250413, 0.00249954, 0.0025    , 0.00249317, 0.00248839,\n",
       "       0.00250168, 0.00250567, 0.00249468, 0.00249486, 0.00250575,\n",
       "       0.00250891, 0.00249133, 0.00249641, 0.00250084, 0.00249707,\n",
       "       0.00250795, 0.00249504, 0.00250747, 0.00248964, 0.00250118,\n",
       "       0.00250986, 0.00248862, 0.00249933, 0.00249319, 0.00249495,\n",
       "       0.0025114 , 0.00250864, 0.00250098, 0.00249746, 0.00250333,\n",
       "       0.00251016, 0.002509  , 0.00250212, 0.00250932, 0.00249664,\n",
       "       0.00249495, 0.00249954, 0.00250008, 0.00250752, 0.00250584,\n",
       "       0.0024935 , 0.0025023 , 0.00250323, 0.0025042 , 0.0024946 ,\n",
       "       0.00249808, 0.00249112, 0.00249994, 0.002506  , 0.00250547,\n",
       "       0.00249872, 0.00250728, 0.00250329, 0.00251151, 0.0025069 ,\n",
       "       0.00249209, 0.00251188, 0.00249384, 0.00250531, 0.00249829,\n",
       "       0.00251118, 0.00250664, 0.00250849, 0.00251192, 0.0025051 ,\n",
       "       0.00248784, 0.00250412, 0.00248792, 0.00248869, 0.00248905,\n",
       "       0.00251035, 0.00249532, 0.00249401, 0.00250802, 0.00250895,\n",
       "       0.0024958 , 0.00249016, 0.00250129, 0.00250909, 0.00249831,\n",
       "       0.00249122, 0.00249129, 0.00249435, 0.00250271, 0.0024892 ,\n",
       "       0.00249921, 0.00250544, 0.0024902 , 0.00250279, 0.00249918,\n",
       "       0.00250445, 0.00249205, 0.00249747, 0.00251219, 0.00249372,\n",
       "       0.00250503, 0.00250141, 0.00250145, 0.00251033, 0.00248863,\n",
       "       0.00250501, 0.00249534, 0.00249817, 0.00249766, 0.00251064,\n",
       "       0.00249023, 0.00249007, 0.00251113, 0.00250333, 0.00250672,\n",
       "       0.00248901, 0.00249995, 0.0024962 , 0.00250806, 0.00249093,\n",
       "       0.00248968, 0.00248767, 0.00248791, 0.00249723, 0.00248762,\n",
       "       0.00250876, 0.00250133, 0.00249095, 0.00250951, 0.00249904,\n",
       "       0.0025006 , 0.00248836, 0.00250532, 0.00250998, 0.00249034,\n",
       "       0.00250723, 0.00250601, 0.00249741, 0.00249515, 0.00250535,\n",
       "       0.00250563, 0.00249074, 0.00249701, 0.00251086, 0.00248807,\n",
       "       0.00250055, 0.00250329, 0.0024992 , 0.00249158, 0.00250864,\n",
       "       0.00249358, 0.00250055, 0.00249453, 0.00249944, 0.00250258,\n",
       "       0.00250191, 0.00249131, 0.00249687, 0.00250846, 0.00249248,\n",
       "       0.00250291, 0.00249606, 0.00250729, 0.00248904, 0.00249366,\n",
       "       0.00249731, 0.00250686, 0.00249986, 0.00249993, 0.00249755,\n",
       "       0.00249038, 0.00250012, 0.00250628, 0.00249178, 0.00250827,\n",
       "       0.00250101, 0.00249866, 0.00249666, 0.0024995 , 0.00249061,\n",
       "       0.0024975 , 0.00249741, 0.00250973, 0.00249889, 0.00248776,\n",
       "       0.00250916, 0.00249466, 0.00249399, 0.00249371, 0.00250196,\n",
       "       0.0025091 , 0.00249752, 0.00248762, 0.00250583, 0.00251212,\n",
       "       0.00249539, 0.00249204, 0.00250824, 0.0025126 , 0.00250671,\n",
       "       0.00249474, 0.00250633, 0.00250471, 0.00249645, 0.00250661,\n",
       "       0.00249507, 0.00250516, 0.00249594, 0.00250044, 0.00249285,\n",
       "       0.00249775, 0.00251112, 0.00250772, 0.00248908, 0.00248774,\n",
       "       0.00250858, 0.00249818, 0.00250879, 0.00250564, 0.00249548,\n",
       "       0.00249693, 0.00250452, 0.0024914 , 0.00249216, 0.00250541,\n",
       "       0.00249696, 0.0024888 , 0.0024991 , 0.00250641, 0.00249402,\n",
       "       0.00251042, 0.00249835, 0.00250509, 0.00249408, 0.00249053,\n",
       "       0.00251235, 0.00248852, 0.00250252, 0.00249078, 0.00248817,\n",
       "       0.00249592, 0.00249678, 0.00251189, 0.00250417, 0.00249646,\n",
       "       0.00250496, 0.00250512, 0.00249216, 0.00249311, 0.00249483,\n",
       "       0.00249987, 0.00250033, 0.00249483, 0.00250699, 0.00249708,\n",
       "       0.00250463, 0.00249233, 0.00250411, 0.00250847, 0.00249865,\n",
       "       0.00249597, 0.00249238, 0.0025058 , 0.00251249, 0.00250878,\n",
       "       0.00249294, 0.00249469, 0.00250568, 0.00248882, 0.00248859,\n",
       "       0.00250538, 0.00250939, 0.00248868, 0.00250672, 0.00250501,\n",
       "       0.0024945 , 0.00249895, 0.00250049, 0.00249899, 0.00251088,\n",
       "       0.00249566, 0.00249769, 0.00250444, 0.00249426, 0.00248969,\n",
       "       0.0024998 , 0.00249609, 0.00249793, 0.00249051, 0.00249084,\n",
       "       0.00251189, 0.00250965, 0.00248887, 0.00250553, 0.00249274,\n",
       "       0.0025109 , 0.00250807, 0.00250725, 0.00250766, 0.00250866,\n",
       "       0.00251215, 0.00250603, 0.00250613, 0.00250792, 0.00250941,\n",
       "       0.00250968, 0.00248796, 0.00250203, 0.0025051 , 0.0024994 ,\n",
       "       0.00248923, 0.00249017, 0.0024924 , 0.00250435, 0.00249266,\n",
       "       0.00250757, 0.00249397, 0.00250282, 0.00250397, 0.00249451,\n",
       "       0.00249294, 0.00249312, 0.00248841, 0.00250362, 0.0024972 ,\n",
       "       0.00250743, 0.00251107, 0.00249329, 0.00249644, 0.00249668,\n",
       "       0.00249245, 0.00251005, 0.00249178, 0.00250479, 0.00249718])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = np.random.uniform(0, 1, size=len(transitions))\n",
    "prob_dist = softmax(logits, tau=100)\n",
    "prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98801\n",
      "101007\n"
     ]
    }
   ],
   "source": [
    "sampled_indices = np.random.choice(\n",
    "    transitions, size=40_000_000, p=prob_dist, replace=True\n",
    ")\n",
    "sampled_indices\n",
    "\n",
    "counts = Counter(sampled_indices)\n",
    "\n",
    "minimum_freq = min(counts.values())\n",
    "maximum_freq = max(counts.values())\n",
    "\n",
    "print(minimum_freq)\n",
    "print(maximum_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({163: 5,\n",
       "         276: 5,\n",
       "         44: 4,\n",
       "         182: 4,\n",
       "         77: 4,\n",
       "         257: 4,\n",
       "         59: 4,\n",
       "         267: 3,\n",
       "         362: 3,\n",
       "         269: 3,\n",
       "         105: 3,\n",
       "         252: 3,\n",
       "         282: 3,\n",
       "         283: 3,\n",
       "         62: 3,\n",
       "         324: 3,\n",
       "         3: 3,\n",
       "         94: 3,\n",
       "         110: 3,\n",
       "         399: 3,\n",
       "         219: 3,\n",
       "         26: 3,\n",
       "         294: 3,\n",
       "         370: 3,\n",
       "         256: 3,\n",
       "         325: 3,\n",
       "         58: 3,\n",
       "         205: 3,\n",
       "         36: 3,\n",
       "         13: 3,\n",
       "         209: 3,\n",
       "         347: 3,\n",
       "         96: 2,\n",
       "         106: 2,\n",
       "         303: 2,\n",
       "         321: 2,\n",
       "         5: 2,\n",
       "         208: 2,\n",
       "         338: 2,\n",
       "         162: 2,\n",
       "         87: 2,\n",
       "         48: 2,\n",
       "         118: 2,\n",
       "         119: 2,\n",
       "         53: 2,\n",
       "         297: 2,\n",
       "         359: 2,\n",
       "         349: 2,\n",
       "         144: 2,\n",
       "         150: 2,\n",
       "         291: 2,\n",
       "         389: 2,\n",
       "         142: 2,\n",
       "         253: 2,\n",
       "         232: 2,\n",
       "         311: 2,\n",
       "         272: 2,\n",
       "         225: 2,\n",
       "         207: 2,\n",
       "         185: 2,\n",
       "         181: 2,\n",
       "         360: 2,\n",
       "         115: 2,\n",
       "         126: 2,\n",
       "         14: 2,\n",
       "         107: 2,\n",
       "         65: 2,\n",
       "         231: 2,\n",
       "         155: 2,\n",
       "         84: 2,\n",
       "         47: 2,\n",
       "         322: 2,\n",
       "         68: 2,\n",
       "         180: 2,\n",
       "         188: 2,\n",
       "         339: 2,\n",
       "         368: 2,\n",
       "         175: 2,\n",
       "         32: 2,\n",
       "         194: 2,\n",
       "         135: 2,\n",
       "         190: 2,\n",
       "         213: 2,\n",
       "         214: 2,\n",
       "         33: 2,\n",
       "         307: 2,\n",
       "         280: 2,\n",
       "         316: 2,\n",
       "         7: 2,\n",
       "         373: 2,\n",
       "         8: 2,\n",
       "         295: 2,\n",
       "         341: 2,\n",
       "         104: 2,\n",
       "         217: 2,\n",
       "         97: 2,\n",
       "         98: 2,\n",
       "         212: 2,\n",
       "         239: 2,\n",
       "         270: 2,\n",
       "         116: 2,\n",
       "         312: 2,\n",
       "         171: 2,\n",
       "         372: 2,\n",
       "         151: 2,\n",
       "         274: 2,\n",
       "         358: 1,\n",
       "         243: 1,\n",
       "         24: 1,\n",
       "         29: 1,\n",
       "         227: 1,\n",
       "         352: 1,\n",
       "         146: 1,\n",
       "         165: 1,\n",
       "         45: 1,\n",
       "         132: 1,\n",
       "         293: 1,\n",
       "         361: 1,\n",
       "         176: 1,\n",
       "         374: 1,\n",
       "         258: 1,\n",
       "         145: 1,\n",
       "         61: 1,\n",
       "         375: 1,\n",
       "         268: 1,\n",
       "         387: 1,\n",
       "         125: 1,\n",
       "         222: 1,\n",
       "         16: 1,\n",
       "         69: 1,\n",
       "         67: 1,\n",
       "         103: 1,\n",
       "         172: 1,\n",
       "         273: 1,\n",
       "         220: 1,\n",
       "         386: 1,\n",
       "         285: 1,\n",
       "         63: 1,\n",
       "         147: 1,\n",
       "         345: 1,\n",
       "         189: 1,\n",
       "         365: 1,\n",
       "         300: 1,\n",
       "         35: 1,\n",
       "         128: 1,\n",
       "         71: 1,\n",
       "         6: 1,\n",
       "         17: 1,\n",
       "         60: 1,\n",
       "         348: 1,\n",
       "         379: 1,\n",
       "         148: 1,\n",
       "         290: 1,\n",
       "         72: 1,\n",
       "         355: 1,\n",
       "         169: 1,\n",
       "         82: 1,\n",
       "         319: 1,\n",
       "         377: 1,\n",
       "         92: 1,\n",
       "         54: 1,\n",
       "         218: 1,\n",
       "         318: 1,\n",
       "         134: 1,\n",
       "         66: 1,\n",
       "         391: 1,\n",
       "         21: 1,\n",
       "         51: 1,\n",
       "         333: 1,\n",
       "         230: 1,\n",
       "         382: 1,\n",
       "         245: 1,\n",
       "         226: 1,\n",
       "         95: 1,\n",
       "         184: 1,\n",
       "         117: 1,\n",
       "         19: 1,\n",
       "         93: 1,\n",
       "         289: 1,\n",
       "         383: 1,\n",
       "         11: 1,\n",
       "         160: 1,\n",
       "         133: 1,\n",
       "         396: 1,\n",
       "         250: 1,\n",
       "         342: 1,\n",
       "         323: 1,\n",
       "         376: 1,\n",
       "         264: 1,\n",
       "         127: 1,\n",
       "         357: 1,\n",
       "         281: 1,\n",
       "         111: 1,\n",
       "         120: 1,\n",
       "         251: 1,\n",
       "         229: 1,\n",
       "         260: 1,\n",
       "         284: 1,\n",
       "         235: 1,\n",
       "         50: 1,\n",
       "         385: 1,\n",
       "         277: 1,\n",
       "         173: 1,\n",
       "         332: 1,\n",
       "         0: 1,\n",
       "         78: 1,\n",
       "         25: 1,\n",
       "         154: 1,\n",
       "         75: 1,\n",
       "         310: 1,\n",
       "         167: 1,\n",
       "         314: 1,\n",
       "         350: 1,\n",
       "         242: 1,\n",
       "         158: 1,\n",
       "         122: 1,\n",
       "         200: 1,\n",
       "         138: 1,\n",
       "         395: 1,\n",
       "         320: 1,\n",
       "         353: 1,\n",
       "         336: 1,\n",
       "         369: 1,\n",
       "         15: 1,\n",
       "         196: 1,\n",
       "         246: 1,\n",
       "         271: 1,\n",
       "         112: 1,\n",
       "         41: 1,\n",
       "         302: 1,\n",
       "         326: 1,\n",
       "         388: 1,\n",
       "         37: 1,\n",
       "         46: 1,\n",
       "         398: 1,\n",
       "         31: 1,\n",
       "         89: 1,\n",
       "         298: 1,\n",
       "         23: 1,\n",
       "         371: 1,\n",
       "         296: 1,\n",
       "         192: 1,\n",
       "         183: 1,\n",
       "         18: 1,\n",
       "         203: 1,\n",
       "         210: 1,\n",
       "         130: 1,\n",
       "         100: 1,\n",
       "         157: 1,\n",
       "         90: 1,\n",
       "         247: 1,\n",
       "         141: 1,\n",
       "         378: 1})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([267, 362,  96, 106, 358, 243, 303,  44, 321,   5, 182, 208,  24,\n",
       "        29, 338, 269, 162, 321, 105, 227, 352, 146, 165,  45,  87, 132,\n",
       "       293, 361, 252,  48, 118, 176, 119, 374, 258,  53, 145,  61, 375,\n",
       "       282, 268, 387, 297, 125, 222,  87, 359, 182, 282, 349, 144, 163,\n",
       "       267, 283,  62, 150, 324,   3,  16,  69,  94, 110, 291, 389, 142,\n",
       "       253, 232, 399,  67, 103, 219, 172, 359, 311, 208, 272, 273, 225,\n",
       "       207,  53, 185, 220, 386, 181, 285,  63, 147, 324, 345, 360, 269,\n",
       "       225, 189, 115, 365, 219, 300,  77, 126, 126,  26, 294,  35, 389,\n",
       "       128,  71,   6,  14, 107, 370,  17,  60, 348, 144,  65, 231, 379,\n",
       "       148, 155,  84, 115, 290,  47,  72,   5, 322, 355,  68, 180, 272,\n",
       "       256, 169,  82, 182, 319, 325, 256, 377,  92, 399,  54,  48, 188,\n",
       "       218, 283, 318, 338, 134, 339, 368, 252,  66, 175, 370,  32, 391,\n",
       "        44, 231,  21, 349, 163,  51, 333,  44, 297, 311, 230, 382, 245,\n",
       "       267, 322, 325,  77,  84, 194, 226, 135, 190,  95, 184, 163, 213,\n",
       "       214, 362, 117, 142,  19, 257, 291,  33,  93, 289, 383, 307,  58,\n",
       "        11, 160, 133, 276, 205, 280,  14, 316,   7, 190, 396, 373,  36,\n",
       "         3, 250, 362,  13, 294,  62, 214,  59, 342,  36, 323,  32,   8,\n",
       "       295, 209, 376, 264, 127, 341, 357, 104, 105, 217, 217,  97, 280,\n",
       "         7,  97,  94,  47,  65,  98, 207, 119, 281, 180, 175, 111, 276,\n",
       "       120, 150, 370, 253, 256, 251, 212, 303,  59, 209, 229,  36, 239,\n",
       "       339, 260, 284, 155, 105,  58, 212, 307,  59, 185, 209,  26,  13,\n",
       "       324, 163, 282, 235, 399, 270,  58, 295,  50, 385, 116, 277, 188,\n",
       "       173, 332,   8,   0, 312,  78, 107, 163,  25, 252, 154, 171,  75,\n",
       "       239, 205, 310, 110, 167,  98, 314,  96, 232, 116, 373, 350, 242,\n",
       "       158, 122, 135, 110, 200,  33, 372, 138, 395, 270, 194,  59, 320,\n",
       "       276, 312, 353, 336, 369,  15, 196, 246,   3, 271, 112,  41, 257,\n",
       "       302, 341, 326, 151, 182, 368, 325, 388, 257, 274, 276,  62,  37,\n",
       "        46, 219, 106, 398,  44,  31, 347, 283, 181,  89, 298,  94,  77,\n",
       "        23, 276, 371, 296, 192, 347,  77, 183,  18, 104, 269, 316, 274,\n",
       "       203, 205, 210, 372, 257, 130, 100, 213, 294, 157, 118,  90,  13,\n",
       "        26,  68, 247, 171, 347, 141, 360, 151, 378, 162])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_next_q_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmax_next_q_values\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m~\u001b[39mdone\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_next_q_values' is not defined"
     ]
    }
   ],
   "source": [
    "max_next_q_values * (~done.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3246],\n",
       "        [-0.7336],\n",
       "        [ 0.6904],\n",
       "        [ 0.0315],\n",
       "        [ 1.4223],\n",
       "        [ 0.0071],\n",
       "        [-0.0206],\n",
       "        [-1.7084],\n",
       "        [ 0.1359],\n",
       "        [ 1.1714],\n",
       "        [ 0.1897],\n",
       "        [ 0.0509],\n",
       "        [ 1.7091],\n",
       "        [ 0.0664],\n",
       "        [-1.0759],\n",
       "        [ 0.8194],\n",
       "        [-1.2782],\n",
       "        [-0.4596],\n",
       "        [ 0.6318],\n",
       "        [-0.0032],\n",
       "        [-1.5888],\n",
       "        [ 0.8822],\n",
       "        [-0.1998],\n",
       "        [-1.2811],\n",
       "        [-1.3465],\n",
       "        [ 0.9563],\n",
       "        [ 0.4550],\n",
       "        [ 0.8518],\n",
       "        [-0.6650],\n",
       "        [ 1.4859],\n",
       "        [-1.2025],\n",
       "        [ 0.2654]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3, 2, 3, 2, 2, 1, 2, 0, 2, 2, 0, 2, 1, 3, 1, 1, 2, 2, 1, 3, 1, 1,\n",
       "        1, 2, 2, 0, 1, 0, 1, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run experiment to find entropy when having tau distributions\n",
    "\n",
    "seed_everything(run_id)\n",
    "env = make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n",
    "\n",
    "states = list(set([s for s, _ in env.mdp.keys()]))\n",
    "actions = list(set([a for _, a in env.mdp.keys()]))\n",
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "\n",
    "transitions_train, transitions_val = generate_train_test_split_with_valid_path(\n",
    "    transitions_list=transitions_list,\n",
    "    start_state=start_state,\n",
    "    terminal_states=terminal_states,\n",
    "    seed=run_id,\n",
    ")\n",
    "\n",
    "train_dataset_transitions = generate_transitions_observations(\n",
    "    transitions_train, num_steps, tau=tau, min_samples=min_samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from liftoff import parse_opts\n",
    "from argparse import Namespace\n",
    "from experiment_src import *\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(\".\"))))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from experiment_src import (\n",
    "    train_net_with_value_function_approximation,\n",
    "    generate_random_policy_transitions,\n",
    "    generate_transitions_observations\n",
    ")\n",
    "from experiments.experiment_utils import setup_logger, seed_everything\n",
    "from overfitting.src.policy_iteration import random_policy_evaluation_q_stochastic\n",
    "from overfitting.src.utils import (\n",
    "    create_random_policy,\n",
    "    extract_V_from_Q_for_stochastic_policy,\n",
    ")\n",
    "from overfitting.src.visualize import draw_simple_gridworld\n",
    "from experiment_src import generate_train_test_split_with_valid_path, check_path_existence_to_any_terminal\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences_and_compute_percentage(\n",
    "    sampled_transitions_list, total_unique_transitions, N\n",
    "):\n",
    "    # Count occurrences of each index in the sampled list\n",
    "    occurrences_count = {}\n",
    "    for index in sampled_transitions_list:\n",
    "        occurrences_count[index] = occurrences_count.get(index, 0) + 1\n",
    "\n",
    "    # Compute the number of indexes that appear at least N times\n",
    "    at_least_N = sum(1 for count in occurrences_count.values() if count >= N)\n",
    "\n",
    "    # Compute the percentage relative to the total number of unique transitions\n",
    "    percentage = (at_least_N / total_unique_transitions) * 100\n",
    "    return percentage, occurrences_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "rows, cols = 10, 10\n",
    "start_state = (0, 0)\n",
    "terminal_states = {(rows - 2, cols - 2): 1.0}\n",
    "p_success = 1\n",
    "seed = 3\n",
    "\n",
    "num_steps = 40_000\n",
    "min_samples = 20\n",
    "# min_samples = 0\n",
    "\n",
    "# Learning hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.05  # Convergence criterion\n",
    "tau = 100\n",
    "batch_size = 32\n",
    "train_max_iterations = 50\n",
    "theta = 1e-6\n",
    "\n",
    "env = make_env(rows, cols, start_state, p_success, terminal_states, seed)\n",
    "\n",
    "states = list(set([s for s, _ in env.mdp.keys()]))\n",
    "actions = list(set([a for _, a in env.mdp.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1000\n",
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "transitions_train, transitions_val = train_test_split(\n",
    "    transitions_list, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "random_policy_transitions = generate_transitions_observations(\n",
    "    transitions_list,\n",
    "    num_steps,\n",
    "    tau=tau,\n",
    "    min_samples=min_samples,\n",
    ")\n",
    "\n",
    "\n",
    "### Training\n",
    "input_size = len(states[0])  # Or another way to represent the size of your input\n",
    "output_size = len(actions)\n",
    "\n",
    "# Initialize the DQN\n",
    "qnet_random_policy = QNET(input_size, output_size)\n",
    "\n",
    "# loss_record_random_policy = train_net_with_value_function_approximation(\n",
    "#     qnet_random_policy,\n",
    "#     random_policy_transitions,\n",
    "#     states,\n",
    "#     actions,\n",
    "#     gamma,\n",
    "#     epsilon,\n",
    "#     batch_size,\n",
    "#     train_max_iterations,\n",
    "#     logger,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_net_with_neural_fitted_q_scaled_loss(\n",
    "#     net,\n",
    "#     transitions,\n",
    "#     Q_pi_random,\n",
    "#     states,\n",
    "#     actions,\n",
    "#     gamma,\n",
    "#     epsilon,\n",
    "#     batch_size,\n",
    "#     max_iterations,\n",
    "#     logger=None,\n",
    "# ):\n",
    "\n",
    "max_iterations = 10\n",
    "transitions = random_policy_transitions\n",
    "\n",
    "net = QNET(input_size, output_size)\n",
    "if logger is None:\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "net.train()\n",
    "\n",
    "transitions_for_counting = [(s, a, ns, r, int(d)) for s, a, ns, r, d, _ in transitions]\n",
    "transition_counts = Counter(transitions_for_counting)\n",
    "\n",
    "# Calculate expected frequency under uniform distribution\n",
    "N_total = len(transitions)\n",
    "N_unique = len(set(transitions_for_counting))\n",
    "expected_frequency = N_total / N_unique\n",
    "\n",
    "# Compute scaling factor relative to uniform distribution\n",
    "inverse_frequency_scaling = {\n",
    "    t: expected_frequency / count for t, count in transition_counts.items()\n",
    "}\n",
    "\n",
    "dataset = TransitionDataset(transitions)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(max_iterations):\n",
    "    total_loss = 0\n",
    "    for state, action, next_state, reward, done in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        q_values = net(state)\n",
    "        next_q_values = net(next_state)\n",
    "        max_next_q_values = next_q_values.detach().max(1)[0].unsqueeze(1)\n",
    "        target_q_values = reward.unsqueeze(1) + gamma * max_next_q_values * (\n",
    "            ~done.unsqueeze(1)\n",
    "        )\n",
    "        target_q_values = torch.where(\n",
    "            done.unsqueeze(1), reward.unsqueeze(1), target_q_values\n",
    "        )\n",
    "\n",
    "        scaled_losses = torch.zeros(size=(len(reward), 1), device=q_values.device)\n",
    "        for i, trans in enumerate(zip(state, action, next_state, reward, done)):\n",
    "            transition = tuple(trans[:5])\n",
    "            scale_factor = inverse_frequency_scaling.get(transition, 1.0)\n",
    "            loss = loss_fn(q_values[i, action[i]].unsqueeze(0), target_q_values[i])\n",
    "            scaled_losses[i] = loss * scale_factor\n",
    "\n",
    "        loss = scaled_losses.mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_record.append((epoch, total_loss, None))\n",
    "\n",
    "logger.info(f\"Exiting after {epoch + 1} epochs with total loss {total_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

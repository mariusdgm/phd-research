{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chainsword\\AppData\\Local\\Temp\\ipykernel_12336\\508223408.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from liftoff import parse_opts\n",
    "from argparse import Namespace\n",
    "from experiment_src import *\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(\".\"))))\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "from experiment_src import (\n",
    "    train_net_with_value_function_approximation,\n",
    "    generate_random_policy_transitions,\n",
    "    generate_transitions_observations,\n",
    "    TransitionDatasetWithScaling\n",
    ")\n",
    "from experiments.experiment_utils import setup_logger, seed_everything\n",
    "from overfitting.src.policy_iteration import random_policy_evaluation_q_stochastic\n",
    "from overfitting.src.utils import (\n",
    "    create_random_policy,\n",
    "    extract_V_from_Q_for_stochastic_policy,\n",
    ")\n",
    "from overfitting.src.visualize import draw_simple_gridworld\n",
    "from experiment_src import generate_train_test_split_with_valid_path, check_path_existence_to_any_terminal\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_occurrences_and_compute_percentage(\n",
    "    sampled_transitions_list, total_unique_transitions, N\n",
    "):\n",
    "    # Count occurrences of each index in the sampled list\n",
    "    occurrences_count = {}\n",
    "    for index in sampled_transitions_list:\n",
    "        occurrences_count[index] = occurrences_count.get(index, 0) + 1\n",
    "\n",
    "    # Compute the number of indexes that appear at least N times\n",
    "    at_least_N = sum(1 for count in occurrences_count.values() if count >= N)\n",
    "\n",
    "    # Compute the percentage relative to the total number of unique transitions\n",
    "    percentage = (at_least_N / total_unique_transitions) * 100\n",
    "    return percentage, occurrences_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "rows, cols = 10, 10\n",
    "start_state = (0, 0)\n",
    "terminal_states = {(rows - 2, cols - 2): 1.0}\n",
    "p_success = 1\n",
    "seed = 3\n",
    "\n",
    "num_steps = 40_000\n",
    "min_samples = 20\n",
    "# min_samples = 0\n",
    "\n",
    "# Learning hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.05  # Convergence criterion\n",
    "tau = 100\n",
    "batch_size = 32\n",
    "train_max_iterations = 50\n",
    "theta = 1e-6\n",
    "\n",
    "env = make_env(rows, cols, start_state, p_success, terminal_states, seed)\n",
    "\n",
    "states = list(set([s for s, _ in env.mdp.keys()]))\n",
    "actions = list(set([a for _, a in env.mdp.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 1000\n",
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "transitions_train, transitions_val = train_test_split(\n",
    "    transitions_list, test_size=0.2, random_state=seed\n",
    ")\n",
    "\n",
    "random_policy_transitions = generate_transitions_observations(\n",
    "    transitions_list,\n",
    "    num_steps,\n",
    "    tau=tau,\n",
    "    min_samples=min_samples,\n",
    ")\n",
    "\n",
    "\n",
    "### Training\n",
    "input_size = len(states[0])  # Or another way to represent the size of your input\n",
    "output_size = len(actions)\n",
    "\n",
    "# Initialize the DQN\n",
    "qnet_random_policy = QNET(input_size, output_size)\n",
    "\n",
    "# loss_record_random_policy = train_net_with_value_function_approximation(\n",
    "#     qnet_random_policy,\n",
    "#     random_policy_transitions,\n",
    "#     states,\n",
    "#     actions,\n",
    "#     gamma,\n",
    "#     epsilon,\n",
    "#     batch_size,\n",
    "#     train_max_iterations,\n",
    "#     logger,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def train_net_with_neural_fitted_q_scaled_loss(\n",
    "# #     net,\n",
    "# #     transitions,\n",
    "# #     Q_pi_random,\n",
    "# #     states,\n",
    "# #     actions,\n",
    "# #     gamma,\n",
    "# #     epsilon,\n",
    "# #     batch_size,\n",
    "# #     max_iterations,\n",
    "# #     logger=None,\n",
    "# # ):\n",
    "\n",
    "# max_iterations = 10\n",
    "# transitions = random_policy_transitions\n",
    "\n",
    "# net = QNET(input_size, output_size)\n",
    "# if logger is None:\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "# net.train()\n",
    "\n",
    "# transitions_for_counting = [\n",
    "#     (s, a, ns, r, int(d)) for s, a, ns, r, d, _ in transitions\n",
    "# ]\n",
    "# transition_counts = Counter(transitions_for_counting)\n",
    "\n",
    "# # Calculate expected frequency under uniform distribution\n",
    "# N_total = len(transitions)\n",
    "# N_unique = len(set(transitions_for_counting))\n",
    "# expected_frequency = N_total / N_unique\n",
    "\n",
    "# # Compute scaling factor relative to uniform distribution\n",
    "# inverse_frequency_scaling = {\n",
    "#     t: expected_frequency / count for t, count in transition_counts.items()\n",
    "# }\n",
    "\n",
    "# dataset = TransitionDatasetWithScaling(transitions, inverse_frequency_scaling)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "# loss_fn = nn.MSELoss(reduction=\"none\")\n",
    "# loss_record = []\n",
    "\n",
    "# for epoch in range(max_iterations):\n",
    "#     total_loss = 0\n",
    "#     for state, action, next_state, reward, done, scale_factor in dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         q_values = net(state)\n",
    "#         next_q_values = net(next_state)\n",
    "#         max_next_q_values = next_q_values.detach().max(1)[0].unsqueeze(1)\n",
    "#         target_q_values = reward.unsqueeze(1) + gamma * max_next_q_values * (\n",
    "#             ~done.unsqueeze(1)\n",
    "#         )\n",
    "#         target_q_values = torch.where(\n",
    "#             done.unsqueeze(1), reward.unsqueeze(1), target_q_values\n",
    "#         ) # explicit handling of terminal states\n",
    "\n",
    "#         individual_losses = loss_fn(q_values.gather(1, action.unsqueeze(-1)), target_q_values)\n",
    "#         scaled_losses = individual_losses.squeeze() * scale_factor.to(q_values.device)\n",
    "    \n",
    "#         loss = scaled_losses.mean()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         break\n",
    "    \n",
    "#     loss_record.append((epoch, total_loss, 0))\n",
    "    \n",
    "#     break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10\n",
    "transitions = random_policy_transitions\n",
    "\n",
    "net = QNET(input_size, output_size)\n",
    "if logger is None:\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "net.train()\n",
    "dataset = TransitionDataset(transitions)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "loss_record = []\n",
    "\n",
    "for epoch in range(max_iterations):\n",
    "    total_loss = 0\n",
    "    for state, action, next_state, reward, done in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        q_values = net(state)\n",
    "        next_q_values = net(next_state)\n",
    "        max_next_q_values = next_q_values.detach().max(1)[0]\n",
    "        \n",
    "        target_q_values_for_actions = reward + gamma * max_next_q_values * (~done)\n",
    "\n",
    "        action_q_values = q_values.gather(1, action.unsqueeze(-1))\n",
    "         \n",
    "        loss = loss_fn(action_q_values, target_q_values_for_actions.unsqueeze(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target_q_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtarget_q_values\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'target_q_values' is not defined"
     ]
    }
   ],
   "source": [
    "target_q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7741, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action.unsqueeze(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2938e+00,  1.4427e-01, -3.4196e-01, -4.9531e-01],\n",
       "        [-5.5965e-01,  1.5732e-01, -1.1921e-01, -1.1361e-01],\n",
       "        [-9.2899e-01,  1.1963e-01, -6.7880e-02, -8.0473e-01],\n",
       "        [-1.0815e+00,  1.6027e-01, -2.7088e-01, -4.0533e-01],\n",
       "        [-9.2247e-01,  1.2536e-01, -1.3494e-01, -6.8017e-01],\n",
       "        [-1.7271e+00,  1.5155e-01, -5.4521e-01, -1.4430e-01],\n",
       "        [-1.3387e+00,  2.6558e-01, -4.1004e-01, -3.6590e-02],\n",
       "        [-1.1518e+00,  2.8918e-01, -3.6429e-01,  4.8384e-03],\n",
       "        [-1.6530e+00,  3.3030e-01, -5.4903e-01, -2.6282e-02],\n",
       "        [-1.3390e+00,  1.5814e-01, -3.4963e-01, -6.2659e-01],\n",
       "        [-1.1150e+00,  1.6729e-01, -2.7006e-01, -5.3678e-01],\n",
       "        [-8.8873e-01,  1.4126e-01, -2.3497e-01, -1.3536e-01],\n",
       "        [-6.0195e-01,  1.5063e-01, -9.8310e-04, -4.7670e-01],\n",
       "        [-5.3520e-01,  1.4904e-01,  7.8122e-03, -3.8799e-01],\n",
       "        [-1.4364e+00,  1.0457e-01, -3.8646e-01, -4.6702e-01],\n",
       "        [-5.2487e-01,  1.6767e-01, -2.9157e-02, -2.6788e-01],\n",
       "        [-1.6299e+00,  4.7175e-01, -5.7502e-01, -6.1003e-02],\n",
       "        [-1.3387e+00,  2.6558e-01, -4.1004e-01, -3.6590e-02],\n",
       "        [-7.7704e-01,  1.5174e-01, -1.8372e-01, -1.9967e-01],\n",
       "        [-4.9004e-01,  2.9195e-01, -1.1522e-01,  4.0072e-02],\n",
       "        [-9.1221e-01,  1.6401e-01, -1.9698e-01, -4.5045e-01],\n",
       "        [-9.9870e-01,  1.3630e-01, -2.4932e-01, -2.8948e-01],\n",
       "        [-7.2582e-01,  1.2560e-01, -5.4534e-02, -5.3815e-01],\n",
       "        [-1.3755e+00,  1.6481e-01, -3.4701e-01, -7.5710e-01],\n",
       "        [-8.5865e-01,  1.2033e-01, -6.3097e-02, -7.1554e-01],\n",
       "        [-8.2208e-01,  1.5366e-01, -3.1560e-02, -7.3579e-01],\n",
       "        [-8.0180e-01,  3.8339e-01, -2.5688e-01, -2.3551e-03],\n",
       "        [-1.1112e+00,  1.2911e-01, -2.9751e-01, -2.2445e-01],\n",
       "        [-1.5561e+00,  9.4371e-02, -4.2718e-01, -4.1377e-01],\n",
       "        [-1.1290e+00,  4.5303e-01, -3.7686e-01, -4.7140e-02],\n",
       "        [-1.0108e+00,  1.4817e-01, -2.9950e-01, -7.0831e-02],\n",
       "        [-1.1518e+00,  2.8918e-01, -3.6429e-01,  4.8384e-03]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_next_q_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmax_next_q_values\u001b[49m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m~\u001b[39mdone\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_next_q_values' is not defined"
     ]
    }
   ],
   "source": [
    "max_next_q_values * (~done.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3246],\n",
       "        [-0.7336],\n",
       "        [ 0.6904],\n",
       "        [ 0.0315],\n",
       "        [ 1.4223],\n",
       "        [ 0.0071],\n",
       "        [-0.0206],\n",
       "        [-1.7084],\n",
       "        [ 0.1359],\n",
       "        [ 1.1714],\n",
       "        [ 0.1897],\n",
       "        [ 0.0509],\n",
       "        [ 1.7091],\n",
       "        [ 0.0664],\n",
       "        [-1.0759],\n",
       "        [ 0.8194],\n",
       "        [-1.2782],\n",
       "        [-0.4596],\n",
       "        [ 0.6318],\n",
       "        [-0.0032],\n",
       "        [-1.5888],\n",
       "        [ 0.8822],\n",
       "        [-0.1998],\n",
       "        [-1.2811],\n",
       "        [-1.3465],\n",
       "        [ 0.9563],\n",
       "        [ 0.4550],\n",
       "        [ 0.8518],\n",
       "        [-0.6650],\n",
       "        [ 1.4859],\n",
       "        [-1.2025],\n",
       "        [ 0.2654]], grad_fn=<GatherBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values.gather(1, action.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3, 2, 3, 2, 2, 1, 2, 0, 2, 2, 0, 2, 1, 3, 1, 1, 2, 2, 1, 3, 1, 1,\n",
       "        1, 2, 2, 0, 1, 0, 1, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(state).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_rl_algos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

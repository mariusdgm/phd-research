{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "root_dir = os.path.dirname(\n",
    "        os.path.dirname(os.path.realpath(\".\")))\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "from rl_envs_forge.envs.grid_world.grid_world import GridWorld\n",
    "from common.src.distribution_src import ( \n",
    "                                         make_env, \n",
    "                                        randomize_walls_positions, \n",
    "                                        generate_train_test_split_with_valid_path,\n",
    "                                        run_distribution_correction_experiment\n",
    "                                        )\n",
    "from common.src.simple_dqn_agent import AgentDQN\n",
    "\n",
    "from common.src.experiment_utils import (\n",
    "    setup_logger,\n",
    "    convert_from_string,\n",
    "    namespace_to_dict,\n",
    ")\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0), (1, 2)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "terminal_states = {(2, 2): 1}\n",
    "start_state = (0, 0)\n",
    "rows = 5\n",
    "cols = 5\n",
    "seed = 2\n",
    "\n",
    "# random_walls = randomize_walls_positions(rows, cols, start_state, terminal_states, 0.2, seed=seed)\n",
    "random_walls = [(2, 0), (1, 2)]\n",
    "print(random_walls)\n",
    "env = make_env(\n",
    "    rows,\n",
    "    cols,\n",
    "    start_state=start_state,\n",
    "    p_success=1,\n",
    "    terminal_states=terminal_states,\n",
    "    seed=seed,\n",
    "    walls=random_walls,\n",
    ")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 2): 1}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation_space_shape(observation_space):\n",
    "    \"\"\" Extract a shape-like tuple from a tuple of discrete spaces. \"\"\"\n",
    "    return tuple(space.n for space in observation_space.spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(5), Discrete(5))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [1, 0, 2, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def state_to_matrix(state, env):\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract the environment size from walls and terminal states\n",
    "    max_rows = env.rows\n",
    "    max_cols = env.cols\n",
    "    \n",
    "    # Create the matrix\n",
    "    matrix = np.zeros((max_rows, max_cols), dtype=int)\n",
    "    \n",
    "    # Mark walls in the matrix\n",
    "    for wall in env.walls:\n",
    "        matrix[wall[0], wall[1]] = 1  # Use 1 to indicate walls\n",
    "    \n",
    "    # Mark terminal states in the matrix\n",
    "    for terminal, value in env.terminal_states.items():\n",
    "        matrix[terminal[0], terminal[1]] = 2 \n",
    "    \n",
    "    pos = state\n",
    "    matrix[pos[0], pos[1]] = 3  # Use 3 to indicate the agent's position\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "state_to_matrix(env.state, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_shape = get_observation_space_shape(env.observation_space)\n",
    "state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts = next(iter(terminal_states))\n",
    "\n",
    "# for trial in range(100000):\n",
    "#     random_walls = randomize_walls_positions(rows, cols, start_state, terminal_states, 0.2, seed=trial)\n",
    "\n",
    "#     if (start_state in random_walls) or (ts in random_walls):\n",
    "#         raise ValueError(\"start state or terminal state in walls\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "\n",
    "transitions_train, transitions_val = generate_train_test_split_with_valid_path(\n",
    "        transitions_list=transitions_list,\n",
    "        start_state=start_state,\n",
    "        terminal_states=terminal_states,\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_params': {'agent': 'AgentDQN',\n",
       "  'args_': {'batch_size': 32,\n",
       "   'epsilon': {'decay': 250000, 'end': 0.01, 'start': 1.0},\n",
       "   'loss_fcn': 'mse_loss',\n",
       "   'replay_start_size': 50,\n",
       "   'target_model_update_freq': 4000,\n",
       "   'train_step_cnt': 500,\n",
       "   'training_freq': 4,\n",
       "   'validation_enabled': True,\n",
       "   'validation_epsilon': 0.001,\n",
       "   'validation_step_cnt': 250}},\n",
       " 'alpha': 0.1,\n",
       " 'batch_size': 32,\n",
       " 'cfg_id': 0,\n",
       " 'cols': 10,\n",
       " 'experiment': 'experiment_distributions',\n",
       " 'experiment_arguments': {'tau': 0.001},\n",
       " 'full_title': '2024May16-005524_configs_tau=0.001',\n",
       " 'gamma': 0.9,\n",
       " 'min_samples': 10,\n",
       " 'neural_fit_mode': 'mean',\n",
       " 'num_steps': 40000,\n",
       " 'optim': {'args_': {'eps': 0.0003125, 'lr': 0.00025}, 'name': 'Adam'},\n",
       " 'out_dir': '.\\\\results\\\\2024May16-005524_configs\\\\0000_tau_0.001\\\\0',\n",
       " 'p_success': 1,\n",
       " 'replay_buffer': {'action_dim': 1, 'max_size': 1000, 'n_step': 0},\n",
       " 'rows': 10,\n",
       " 'run_id': 0,\n",
       " 'seed': 3426884958,\n",
       " 'start_state': (1, 1),\n",
       " 'tau': 0.001,\n",
       " 'terminal_states': {(8, 8): 1.0},\n",
       " 'title': 'tau=0.001',\n",
       " 'train_max_iterations': 30}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load up a config\n",
    "\n",
    "file_path = r'D:\\Work\\repos\\phd-research\\experiments\\dqn\\results\\2024May16-005524_configs\\0000_tau_0.001\\0\\cfg.yaml'\n",
    "\n",
    "# Open the YAML file and load its content into a dictionary\n",
    "with open(file_path, 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "    \n",
    "opts = Namespace(**data)\n",
    "\n",
    "logger = setup_logger(\n",
    "        opts.full_title\n",
    "    )\n",
    "\n",
    "opts.seed = random.randint(0, 2**32 - 1) if opts.seed is None else opts.seed\n",
    "opts.start_state = convert_from_string(opts.start_state)\n",
    "opts.terminal_states = namespace_to_dict(opts.terminal_states)\n",
    "opts_dict = vars(opts)\n",
    "\n",
    "opts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decay': 250000, 'end': 0.01, 'start': 1.0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts_dict[\"agent_params\"][\"args_\"][\"epsilon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-16 01:34:53,254 - 2024May16-005524_configs_tau=0.001 - INFO - Starting experiment: 2024May16-005524_configs_tau=0.001\n",
      "2024-05-16 01:34:53,261 - 2024May16-005524_configs_tau=0.001 - INFO - Loaded configuration settings.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AgentDQN' object has no attribute 'policy_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m validation_env \u001b[38;5;241m=\u001b[39m env \u001b[38;5;241m=\u001b[39m make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m### Setup output and loading paths ###\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m experiment_agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgentDQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_output_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopts_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopts_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_training_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopts_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_tensorboard_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m experiment_agent\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Work\\repos\\phd-research\\common\\src\\simple_dqn_agent.py:113\u001b[0m, in \u001b[0;36mAgentDQN.__init__\u001b[1;34m(self, train_env, validation_env, experiment_output_folder, experiment_name, resume_training_path, save_checkpoints, logger, config, enable_tensorboard_logging)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m replace_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_config_settings(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m--> 113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# init policy, target and optim\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Set initial values related to training and monitoring\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# frame nr\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Work\\repos\\phd-research\\common\\src\\simple_dqn_agent.py:281\u001b[0m, in \u001b[0;36mAgentDQN._init_models\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;66;03m# TODO: get sizes to reach here\u001b[39;00m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# self.policy_model = QNET(\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m#     input_size, output_size,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m#     estiamtor_name = estimator_settings[\"model\"]\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m#     raise ValueError(f\"Could not setup estimator. Tried with: {estiamtor_name}\")\u001b[39;00m\n\u001b[0;32m    279\u001b[0m optimizer_settings \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptim\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m--> 281\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_model\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptimizer_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    282\u001b[0m )\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialized newtworks and optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AgentDQN' object has no attribute 'policy_model'"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Starting experiment: {opts_dict['full_title']}\")\n",
    "\n",
    "rows = opts_dict[\"rows\"]\n",
    "cols = opts_dict[\"cols\"]\n",
    "start_state = opts_dict[\"start_state\"]\n",
    "p_success = opts_dict[\"p_success\"]\n",
    "terminal_states = opts_dict[\"terminal_states\"]\n",
    "run_id = opts_dict[\"run_id\"]\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = env = make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n",
    "validation_env = env = make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=opts_dict[\"out_dir\"],\n",
    "    experiment_name=opts_dict[\"experiment\"],\n",
    "    resume_training_path=None,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=opts_dict,\n",
    "    enable_tensorboard_logging=False,\n",
    ")\n",
    "\n",
    "experiment_agent.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_records, bm_error = run_distribution_correction_experiment(\n",
    "#         tau=opts.tau,\n",
    "#         seed=opts.seed,\n",
    "#         run_id=opts.run_id,\n",
    "#         rows=opts.rows,\n",
    "#         cols=opts.cols,\n",
    "#         start_state=opts.start_state,\n",
    "#         p_success=opts.p_success,\n",
    "#         terminal_states=opts.terminal_states,\n",
    "#         num_steps=opts.num_steps,\n",
    "#         gamma=opts.gamma,\n",
    "#         min_samples=opts.min_samples,\n",
    "#         batch_size=opts.batch_size,\n",
    "#         train_max_iterations=opts.train_max_iterations,\n",
    "#         neural_fit_mode=opts.neural_fit_mode,\n",
    "#         algorithm=opts.algorithm,\n",
    "#         logger=logger,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

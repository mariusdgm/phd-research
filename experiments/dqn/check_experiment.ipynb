{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "root_dir = os.path.dirname(\n",
    "        os.path.dirname(os.path.realpath(\".\")))\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "from rl_envs_forge.envs.grid_world.grid_world import GridWorld\n",
    "from common.src.distribution_src import make_env, randomize_walls_positions, generate_train_test_split_with_valid_path\n",
    "from common.src.dqn.my_dqn import AgentDQN\n",
    "\n",
    "from common.src.experiment_utils import (\n",
    "    setup_logger,\n",
    "    convert_from_string,\n",
    "    namespace_to_dict,\n",
    ")\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0), (1, 2)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "terminal_states = {(2, 2): 1}\n",
    "start_state = (0, 0)\n",
    "rows = 3\n",
    "cols = 3\n",
    "seed = 2\n",
    "\n",
    "# random_walls = randomize_walls_positions(rows, cols, start_state, terminal_states, 0.2, seed=seed)\n",
    "random_walls = [(2, 0), (1, 2)]\n",
    "print(random_walls)\n",
    "env = make_env(rows, cols, start_state=start_state, p_success=1, terminal_states=terminal_states, seed=seed, walls=random_walls)\n",
    "env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), <Action.UP: 0>): [((0, 0), 0, False, 1)],\n",
       " ((0, 0), <Action.RIGHT: 1>): [((0, 1), 0, False, 1)],\n",
       " ((0, 0), <Action.DOWN: 2>): [((1, 0), 0, False, 1)],\n",
       " ((0, 0), <Action.LEFT: 3>): [((0, 0), 0, False, 1)],\n",
       " ((0, 1), <Action.UP: 0>): [((0, 1), 0, False, 1)],\n",
       " ((0, 1), <Action.RIGHT: 1>): [((0, 2), 0, False, 1)],\n",
       " ((0, 1), <Action.DOWN: 2>): [((1, 1), 0, False, 1)],\n",
       " ((0, 1), <Action.LEFT: 3>): [((0, 0), 0, False, 1)],\n",
       " ((0, 2), <Action.UP: 0>): [((0, 2), 0, False, 1)],\n",
       " ((0, 2), <Action.RIGHT: 1>): [((0, 2), 0, False, 1)],\n",
       " ((0, 2), <Action.DOWN: 2>): [((0, 2), 0, False, 1)],\n",
       " ((0, 2), <Action.LEFT: 3>): [((0, 1), 0, False, 1)],\n",
       " ((1, 0), <Action.UP: 0>): [((0, 0), 0, False, 1)],\n",
       " ((1, 0), <Action.RIGHT: 1>): [((1, 1), 0, False, 1)],\n",
       " ((1, 0), <Action.DOWN: 2>): [((1, 0), 0, False, 1)],\n",
       " ((1, 0), <Action.LEFT: 3>): [((1, 0), 0, False, 1)],\n",
       " ((1, 1), <Action.UP: 0>): [((0, 1), 0, False, 1)],\n",
       " ((1, 1), <Action.RIGHT: 1>): [((1, 1), 0, False, 1)],\n",
       " ((1, 1), <Action.DOWN: 2>): [((2, 1), 0, False, 1)],\n",
       " ((1, 1), <Action.LEFT: 3>): [((1, 0), 0, False, 1)],\n",
       " ((2, 1), <Action.UP: 0>): [((1, 1), 0, False, 1)],\n",
       " ((2, 1), <Action.RIGHT: 1>): [((2, 2), 1, True, 1)],\n",
       " ((2, 1), <Action.DOWN: 2>): [((2, 1), 0, False, 1)],\n",
       " ((2, 1), <Action.LEFT: 3>): [((2, 1), 0, False, 1)]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = next(iter(terminal_states))\n",
    "\n",
    "for trial in range(100000):\n",
    "    random_walls = randomize_walls_positions(rows, cols, start_state, terminal_states, 0.2, seed=trial)\n",
    "\n",
    "    if (start_state in random_walls) or (ts in random_walls):\n",
    "        raise ValueError(\"start state or terminal state in walls\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "\n",
    "transitions_train, transitions_val = generate_train_test_split_with_valid_path(\n",
    "        transitions_list=transitions_list,\n",
    "        start_state=start_state,\n",
    "        terminal_states=terminal_states,\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_params': {'agent': 'AgentDQN',\n",
       "  'args_': {'batch_size': 32,\n",
       "   'epsilon': {'decay': 250000, 'end': 0.01, 'start': 1.0},\n",
       "   'gamma': 0.99,\n",
       "   'loss_fcn': 'mse_loss',\n",
       "   'replay_start_size': 5000,\n",
       "   'target_model_update_freq': 4000,\n",
       "   'train_step_cnt': 200000,\n",
       "   'training_freq': 4,\n",
       "   'validation_enabled': True,\n",
       "   'validation_epsilon': 0.001,\n",
       "   'validation_step_cnt': 125000}},\n",
       " 'alpha': 0.1,\n",
       " 'batch_size': 32,\n",
       " 'cfg_id': 0,\n",
       " 'cols': 10,\n",
       " 'epsilon': 0.01,\n",
       " 'estimator': {'args_': {'conv_hidden_out_size': 32,\n",
       "   'lin_hidden_out_size': 128},\n",
       "  'model': 'Conv_QNET'},\n",
       " 'experiment': 'experiment_distributions',\n",
       " 'experiment_arguments': {'tau': 0.001},\n",
       " 'full_title': '2024May07-003612_configs_tau=0.001',\n",
       " 'gamma': 0.9,\n",
       " 'min_samples': 10,\n",
       " 'neural_fit_mode': 'mean',\n",
       " 'num_steps': 40000,\n",
       " 'optim': {'args_': {'eps': 0.0003125, 'lr': 0.00025}, 'name': 'Adam'},\n",
       " 'out_dir': '.\\\\results\\\\2024May07-003612_configs\\\\0000_tau_0.001\\\\0',\n",
       " 'p_success': 1,\n",
       " 'redo': {'attach': False,\n",
       "  'beta': 1,\n",
       "  'enabled': False,\n",
       "  'redo_freq': 1000,\n",
       "  'selection_option': None,\n",
       "  'tau': 0.1},\n",
       " 'replay_buffer': {'action_dim': 1, 'max_size': 100000, 'n_step': 0},\n",
       " 'reward_perception': None,\n",
       " 'rows': 10,\n",
       " 'run_id': 0,\n",
       " 'seed': 1021853065,\n",
       " 'start_state': (1, 1),\n",
       " 'tau': 0.001,\n",
       " 'terminal_states': {(8, 8): 1.0},\n",
       " 'title': 'tau=0.001',\n",
       " 'train_max_iterations': 30}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Setup output and loading paths ###\n",
    "file_path = r'results\\2024May07-003612_configs\\0000_tau_0.001\\0\\cfg.yaml'\n",
    "\n",
    "# Open the YAML file and load its content into a dictionary\n",
    "with open(file_path, 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "    \n",
    "opts = Namespace(**data)\n",
    "\n",
    "opts.seed = random.randint(0, 2**32 - 1) if opts.seed is None else opts.seed\n",
    "opts.start_state = convert_from_string(opts.start_state)\n",
    "opts.terminal_states = namespace_to_dict(opts.terminal_states)\n",
    "config = vars(opts)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m train_env \u001b[38;5;241m=\u001b[39m env \u001b[38;5;241m=\u001b[39m make_env(rows, cols, start_state, p_success, terminal_states, seed)\n\u001b[0;32m      3\u001b[0m validation_env \u001b[38;5;241m=\u001b[39m env \u001b[38;5;241m=\u001b[39m make_env(rows, cols, start_state, p_success, terminal_states, seed)\n\u001b[1;32m----> 8\u001b[0m experiment_agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgentDQN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_output_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mout_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_training_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_checkpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_tensorboard_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# experiment_agent.train(train_epochs=config[\"epochs_to_train\"])\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Work\\repos\\phd-research\\common\\src\\dqn\\my_dqn.py:106\u001b[0m, in \u001b[0;36mAgentDQN.__init__\u001b[1;34m(self, train_env, validation_env, experiment_output_folder, experiment_name, resume_training_path, save_checkpoints, logger, config, enable_tensorboard_logging)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m replace_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 106\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_config_settings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_models(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)  \u001b[38;5;66;03m# init policy, target and optim\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Set initial values related to training and monitoring\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Work\\repos\\phd-research\\common\\src\\dqn\\my_dqn.py:214\u001b[0m, in \u001b[0;36mAgentDQN._load_config_settings\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    204\u001b[0m eps_settings \u001b[38;5;241m=\u001b[39m agent_params\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m250_000\u001b[39m}\n\u001b[0;32m    206\u001b[0m )\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_by_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_linear_decay_function(\n\u001b[0;32m    208\u001b[0m     start\u001b[38;5;241m=\u001b[39meps_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    209\u001b[0m     end\u001b[38;5;241m=\u001b[39meps_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    210\u001b[0m     decay\u001b[38;5;241m=\u001b[39meps_settings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    211\u001b[0m     eps_decay_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_start_size,\n\u001b[0;32m    212\u001b[0m )\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_and_init_envs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# sets up in_features etc...\u001b[39;00m\n\u001b[0;32m    216\u001b[0m buffer_settings \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplay_buffer\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100_000\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m}\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_buffer \u001b[38;5;241m=\u001b[39m ReplayBuffer(\n\u001b[0;32m    220\u001b[0m     max_size\u001b[38;5;241m=\u001b[39mbuffer_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m100_000\u001b[39m),\n\u001b[0;32m    221\u001b[0m     state_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features,\n\u001b[0;32m    222\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39mbuffer_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    223\u001b[0m     n_step\u001b[38;5;241m=\u001b[39mbuffer_settings\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m    224\u001b[0m )\n",
      "File \u001b[1;32mD:\\Work\\repos\\phd-research\\common\\src\\dqn\\my_dqn.py:350\u001b[0m, in \u001b[0;36mAgentDQN._read_and_init_envs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    346\u001b[0m state_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    348\u001b[0m \u001b[38;5;66;03m# permute to get batch, channel, w, h shape\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;66;03m# specific to minatar\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m (\u001b[43mstate_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m, state_shape[\u001b[38;5;241m0\u001b[39m], state_shape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "p_success = 1\n",
    "train_env = env = make_env(rows, cols, start_state, p_success, terminal_states, seed)\n",
    "validation_env = env = make_env(rows, cols, start_state, p_success, terminal_states, seed)\n",
    "\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=config[\"out_dir\"],\n",
    "    experiment_name=config[\"experiment\"],\n",
    "    resume_training_path=None,\n",
    "    save_checkpoints=True,\n",
    "    logger=None,\n",
    "    config=config,\n",
    "    enable_tensorboard_logging=False,\n",
    ")\n",
    "\n",
    "# experiment_agent.train(train_epochs=config[\"epochs_to_train\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

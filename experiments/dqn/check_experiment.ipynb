{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chainsword\\AppData\\Local\\Temp\\ipykernel_19596\\881903052.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "\n",
    "root_dir = os.path.dirname(\n",
    "        os.path.dirname(os.path.realpath(\".\")))\n",
    "\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "from rl_envs_forge.envs.grid_world.grid_world import GridWorld\n",
    "from common.src.distribution_src import ( \n",
    "                                         make_env, \n",
    "                                        randomize_walls_positions, \n",
    "                                        generate_train_test_split_with_valid_path,\n",
    "                                        run_distribution_correction_experiment\n",
    "                                        )\n",
    "from common.src.simple_dqn_agent import AgentDQN\n",
    "\n",
    "from common.src.experiment_utils import (\n",
    "    setup_logger,\n",
    "    namespace_to_dict,\n",
    ")\n",
    "\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0), (1, 2)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "terminal_states = {(2, 2): 1}\n",
    "start_state = (2, 1)\n",
    "rows = 5\n",
    "cols = 5\n",
    "seed = 2\n",
    "\n",
    "# random_walls = randomize_walls_positions(rows, cols, start_state, terminal_states, 0.2, seed=seed)\n",
    "random_walls = [(2, 0), (1, 2)]\n",
    "print(random_walls)\n",
    "env = make_env(\n",
    "    rows,\n",
    "    cols,\n",
    "    start_state=start_state,\n",
    "    p_success=1,\n",
    "    terminal_states=terminal_states,\n",
    "    seed=seed,\n",
    "    walls=random_walls,\n",
    ")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observation_space_shape(observation_space):\n",
    "    \"\"\" Extract a shape-like tuple from a tuple of discrete spaces. \"\"\"\n",
    "    return tuple(space.n for space in observation_space.spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(5), Discrete(5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0],\n",
       "       [1, 3, 2, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def state_to_matrix(state, env):\n",
    "    import numpy as np\n",
    "\n",
    "    # Extract the environment size from walls and terminal states\n",
    "    max_rows = env.rows\n",
    "    max_cols = env.cols\n",
    "    \n",
    "    # Create the matrix\n",
    "    matrix = np.zeros((max_rows, max_cols), dtype=int)\n",
    "    \n",
    "    # Mark walls in the matrix\n",
    "    for wall in env.walls:\n",
    "        matrix[wall[0], wall[1]] = 1  # Use 1 to indicate walls\n",
    "    \n",
    "    # Mark terminal states in the matrix\n",
    "    for terminal, value in env.terminal_states.items():\n",
    "        matrix[terminal[0], terminal[1]] = 2 \n",
    "    \n",
    "    pos = state\n",
    "    matrix[pos[0], pos[1]] = 3  # Use 3 to indicate the agent's position\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "state_to_matrix(env.state, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_shape = get_observation_space_shape(env.observation_space)\n",
    "state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts = next(iter(terminal_states))\n",
    "\n",
    "# for trial in range(100000):\n",
    "#     random_walls = randomize_walls_positions(rows, cols, start_state, terminal_states, 0.2, seed=trial)\n",
    "\n",
    "#     if (start_state in random_walls) or (ts in random_walls):\n",
    "#         raise ValueError(\"start state or terminal state in walls\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions_list = [(key[0], key[1], *value[0]) for key, value in env.mdp.items()]\n",
    "\n",
    "transitions_train, transitions_val = generate_train_test_split_with_valid_path(\n",
    "        transitions_list=transitions_list,\n",
    "        start_state=start_state,\n",
    "        terminal_states=terminal_states,\n",
    ")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent_params': {'agent': 'AgentDQN',\n",
       "  'args_': {'batch_size': 32,\n",
       "   'epsilon': {'decay': 250000, 'end': 0.01, 'start': 1.0},\n",
       "   'gamma': 0.9,\n",
       "   'loss_fcn': 'mse_loss',\n",
       "   'replay_start_size': 50,\n",
       "   'target_model_update_freq': 40,\n",
       "   'train_step_cnt': 500,\n",
       "   'training_freq': 4,\n",
       "   'validation_enabled': False,\n",
       "   'validation_epsilon': 0.001,\n",
       "   'validation_step_cnt': 250}},\n",
       " 'algorithm': 'default',\n",
       " 'cfg_id': 0,\n",
       " 'cols': 10,\n",
       " 'experiment': 'experiment_distributions',\n",
       " 'experiment_arguments': {'algorithm': 'default'},\n",
       " 'full_title': '2024May17-105508_configs_algorithm=default',\n",
       " 'neural_fit_mode': 'max',\n",
       " 'num_steps': 40000,\n",
       " 'optim': {'args_': {'eps': 0.0003125, 'lr': 0.1}, 'name': 'Adam'},\n",
       " 'out_dir': '.\\\\results\\\\2024May17-105508_configs\\\\0000_algorithm_default\\\\0',\n",
       " 'p_success': 1,\n",
       " 'replay_buffer': {'action_dim': 1, 'max_size': 1000, 'n_step': 0},\n",
       " 'rows': 10,\n",
       " 'run_id': 0,\n",
       " 'seed': 3261333208,\n",
       " 'start_state': (1, 1),\n",
       " 'terminal_states': {(8, 8): 1.0},\n",
       " 'title': 'algorithm=default',\n",
       " 'train_max_iterations': 30}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load up a config\n",
    "\n",
    "file_path = r'D:\\Work\\repos\\phd-research\\experiments\\dqn\\results\\2024May17-105508_configs\\0000_algorithm_default\\0\\cfg.yaml'\n",
    "\n",
    "# Open the YAML file and load its content into a dictionary\n",
    "with open(file_path, 'r') as file:\n",
    "    data = yaml.safe_load(file)\n",
    "    \n",
    "opts = Namespace(**data)\n",
    "\n",
    "logger = setup_logger(\n",
    "        opts.full_title\n",
    "    )\n",
    "\n",
    "opts.seed = random.randint(0, 2**32 - 1) if opts.seed is None else opts.seed\n",
    "opts_dict = namespace_to_dict(opts)\n",
    "# opts_dict = vars(opts)\n",
    "\n",
    "opts_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(agent_params={'agent': 'AgentDQN', 'args_': {'batch_size': 32, 'epsilon': {'decay': 250000, 'end': 0.01, 'start': 1.0}, 'gamma': 0.9, 'loss_fcn': 'mse_loss', 'replay_start_size': 50, 'target_model_update_freq': 40, 'train_step_cnt': 500, 'training_freq': 4, 'validation_enabled': False, 'validation_epsilon': 0.001, 'validation_step_cnt': 250}}, algorithm='default', cfg_id=0, cols=10, experiment='experiment_distributions', experiment_arguments={'algorithm': 'default'}, full_title='2024May17-105508_configs_algorithm=default', neural_fit_mode='max', num_steps=40000, optim={'args_': {'eps': 0.0003125, 'lr': 0.1}, 'name': 'Adam'}, out_dir='.\\\\results\\\\2024May17-105508_configs\\\\0000_algorithm_default\\\\0', p_success=1, replay_buffer={'action_dim': 1, 'max_size': 1000, 'n_step': 0}, rows=10, run_id=0, seed=3261333208, start_state='(1, 1)', terminal_states={'(8, 8)': 1.0}, title='algorithm=default', train_max_iterations=30)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts_dict[\"start_state\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 8): 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts_dict[\"terminal_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-19 17:51:26,768 - 2024May17-105508_configs_algorithm=default - INFO - Starting experiment: 2024May17-105508_configs_algorithm=default\n",
      "2024-05-19 17:51:26,774 - 2024May17-105508_configs_algorithm=default - INFO - Loaded configuration settings.\n",
      "2024-05-19 17:51:27,315 - 2024May17-105508_configs_algorithm=default - INFO - Initialized newtworks and optimizer.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f\"Starting experiment: {opts_dict['full_title']}\")\n",
    "\n",
    "rows = opts_dict[\"rows\"]\n",
    "cols = opts_dict[\"cols\"]\n",
    "start_state = opts_dict[\"start_state\"]\n",
    "p_success = opts_dict[\"p_success\"]\n",
    "terminal_states = opts_dict[\"terminal_states\"]\n",
    "run_id = opts_dict[\"run_id\"]\n",
    "\n",
    "### Setup environments ###\n",
    "train_env = make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n",
    "validation_env = make_env(rows, cols, start_state, p_success, terminal_states, run_id)\n",
    "\n",
    "### Setup output and loading paths ###\n",
    "\n",
    "experiment_agent = AgentDQN(\n",
    "    train_env=train_env,\n",
    "    validation_env=validation_env,\n",
    "    experiment_output_folder=opts_dict[\"out_dir\"],\n",
    "    experiment_name=opts_dict[\"experiment\"],\n",
    "    resume_training_path=None,\n",
    "    save_checkpoints=True,\n",
    "    logger=logger,\n",
    "    config=opts_dict,\n",
    ")\n",
    "\n",
    "# experiment_agent.train(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(8, 8): 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_env.terminal_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State: (1, 2), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (0, 2), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (0, 1), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (1, 1), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (0, 3), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 2), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 2), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 3), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 3), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (1, 3), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (1, 4), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 4), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 4), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 1), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (4, 1), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (4, 0), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (4, 2), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (5, 2), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (5, 1), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (6, 1), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (6, 2), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (7, 2), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 2), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 3), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (9, 3), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (7, 3), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (6, 3), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (6, 4), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (7, 4), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (5, 4), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 1), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 0), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (5, 0), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (6, 0), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (7, 0), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 0), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (9, 0), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (9, 1), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (7, 1), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 1), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (0, 4), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 5), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 5), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 6), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 7), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 8), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 8), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 9), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (2, 9), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (1, 7), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 7), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (1, 8), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (0, 8), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (0, 7), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (1, 6), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (1, 5), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (3, 6), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (4, 6), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (4, 5), Action: 2, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (4, 4), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (4, 3), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (9, 2), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (9, 4), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (9, 5), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 5), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 6), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 4), Action: 3, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 7), Action: 1, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (7, 7), Action: 0, Reward: 0, Terminated: False, Truncated: False, Info: {}\n",
      "New State: (8, 8), Action: 1, Reward: 1.0, Terminated: True, Truncated: False, Info: {}\n",
      "Reached terminal state or truncated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 3),\n",
       " (2, 2),\n",
       " (3, 2),\n",
       " (3, 3),\n",
       " (2, 3),\n",
       " (1, 3),\n",
       " (1, 4),\n",
       " (2, 4),\n",
       " (3, 4),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (4, 0),\n",
       " (4, 2),\n",
       " (5, 2),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (6, 2),\n",
       " (7, 2),\n",
       " (8, 2),\n",
       " (8, 3),\n",
       " (9, 3),\n",
       " (7, 3),\n",
       " (6, 3),\n",
       " (6, 4),\n",
       " (7, 4),\n",
       " (5, 4),\n",
       " (2, 1),\n",
       " (3, 0),\n",
       " (5, 0),\n",
       " (6, 0),\n",
       " (7, 0),\n",
       " (8, 0),\n",
       " (9, 0),\n",
       " (9, 1),\n",
       " (7, 1),\n",
       " (8, 1),\n",
       " (0, 4),\n",
       " (2, 5),\n",
       " (3, 5),\n",
       " (2, 6),\n",
       " (2, 7),\n",
       " (2, 8),\n",
       " (3, 8),\n",
       " (3, 9),\n",
       " (2, 9),\n",
       " (1, 7),\n",
       " (3, 7),\n",
       " (1, 8),\n",
       " (0, 8),\n",
       " (0, 7),\n",
       " (1, 6),\n",
       " (1, 5),\n",
       " (3, 6),\n",
       " (4, 6),\n",
       " (4, 5),\n",
       " (4, 4),\n",
       " (4, 3),\n",
       " (9, 2),\n",
       " (9, 4),\n",
       " (9, 5),\n",
       " (8, 5),\n",
       " (8, 6),\n",
       " (8, 4),\n",
       " (8, 7),\n",
       " (7, 7),\n",
       " (8, 8)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def randomly_step_through_env(env):\n",
    "    is_terminated = False\n",
    "    truncated = False\n",
    "    visited_states = set()\n",
    "    recorded_states = []\n",
    "\n",
    "    while not is_terminated and not truncated:\n",
    "        # Sample a random action from the action space\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "        # Take a step in the environment\n",
    "        s_prime, reward, is_terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        # Convert state to a hashable type (e.g., tuple) if it's not already\n",
    "        s_prime_hashable = tuple(s_prime) if isinstance(s_prime, (list, np.ndarray)) else s_prime\n",
    "        \n",
    "        # Check if the state is new\n",
    "        if s_prime_hashable not in visited_states:\n",
    "            visited_states.add(s_prime_hashable)\n",
    "            recorded_states.append(s_prime)\n",
    "            print(f\"New State: {s_prime}, Action: {action}, Reward: {reward}, Terminated: {is_terminated}, Truncated: {truncated}, Info: {info}\")\n",
    "\n",
    "    print(\"Reached terminal state or truncated.\")\n",
    "    return recorded_states\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `train_env` is your environment instance\n",
    "randomly_step_through_env(train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_records, bm_error = run_distribution_correction_experiment(\n",
    "#         tau=opts.tau,\n",
    "#         seed=opts.seed,\n",
    "#         run_id=opts.run_id,\n",
    "#         rows=opts.rows,\n",
    "#         cols=opts.cols,\n",
    "#         start_state=opts.start_state,\n",
    "#         p_success=opts.p_success,\n",
    "#         terminal_states=opts.terminal_states,\n",
    "#         num_steps=opts.num_steps,\n",
    "#         gamma=opts.gamma,\n",
    "#         min_samples=opts.min_samples,\n",
    "#         batch_size=opts.batch_size,\n",
    "#         train_max_iterations=opts.train_max_iterations,\n",
    "#         neural_fit_mode=opts.neural_fit_mode,\n",
    "#         algorithm=opts.algorithm,\n",
    "#         logger=logger,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
